{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN       pip install scikit-learn==0.22.2        in terminal first to make sure scikit .22 running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# import libraries and download NLTK\n",
    "import pickle\n",
    "import nltk\n",
    "import torch\n",
    "import sklearn\n",
    "nltk.download(['wordnet','punkt','stopwords'])\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.1\n"
     ]
    }
   ],
   "source": [
    "#confirm sklearn .22 running\n",
    "sklearn_version = sklearn.__version__\n",
    "print(sklearn_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for ETL\n",
    "def ETL_Data():\n",
    "    engine = create_engine('sqlite:///DRP.db')\n",
    "    df = pd.read_sql_table('etl_pipeline', con=engine)\n",
    "    #classification ML model:\n",
    "    X = df.message #messages for X input, REMOVED .values 11/14/21 to 'elbow'\n",
    "    y = df.drop(['id','message','original','genre'],axis=1) #y = classification categories (exclude other columns)\n",
    "    y = y.values\n",
    "    ycol = df.drop(['id','message','original','genre'],axis=1)\n",
    "    ycol = list(ycol.columns)\n",
    "    return X, y, ycol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize with and without stop words\n",
    "def tokenize_w_stopw(text):\n",
    "    \n",
    "    '''function to tokenize messages with and without stop words\n",
    "    \n",
    "    - tokenize words\n",
    "    - lemmatize words\n",
    "    - lowercase words\n",
    "    - remove whitespace\n",
    "    - append clean tokens\n",
    "    - identify stopwords; append\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #var for word tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    #instantiate lemmatizer (used to split into words and convert words to base forms)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "\n",
    "    # iterate through tokenized words and append tokenized and lemmatized words to clean_tokens var\n",
    "    clean_tokens = []\n",
    "    clean_tok_nostop = []\n",
    "    for tok in tokens:\n",
    "        \n",
    "        # lemmatize, normalize/put into lower case + remove leading/trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "        #remove stopwords\n",
    "        if tok not in stopwords.words(\"english\"):\n",
    "            clean_tok_nostop.append(clean_tok)\n",
    "\n",
    "    return clean_tokens, clean_tok_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#check stopwords in English\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNCLEAN MSG:  Weather update - a cold front from Cuba that could pass over Haiti\n",
      "\n",
      "\n",
      "TOKENIZED TXT:  ['weather', 'update', '-', 'a', 'cold', 'front', 'from', 'cuba', 'that', 'could', 'pas', 'over', 'haiti'] \n",
      "\n",
      "TOKENIZED TXT with STOPWORDS:  ['weather', 'update', '-', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti'] \n",
      "\n",
      "\n",
      " -----------------------------------------------------------------------------\n",
      "UNCLEAN MSG:  Is the Hurricane over or is it not over\n",
      "\n",
      "\n",
      "TOKENIZED TXT:  ['is', 'the', 'hurricane', 'over', 'or', 'is', 'it', 'not', 'over'] \n",
      "\n",
      "TOKENIZED TXT with STOPWORDS:  ['is', 'hurricane'] \n",
      "\n",
      "\n",
      " -----------------------------------------------------------------------------\n",
      "UNCLEAN MSG:  Looking for someone but no name\n",
      "\n",
      "\n",
      "TOKENIZED TXT:  ['looking', 'for', 'someone', 'but', 'no', 'name'] \n",
      "\n",
      "TOKENIZED TXT with STOPWORDS:  ['looking', 'someone', 'name'] \n",
      "\n",
      "\n",
      " -----------------------------------------------------------------------------\n",
      "UNCLEAN MSG:  UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately.\n",
      "\n",
      "\n",
      "TOKENIZED TXT:  ['un', 'report', 'leogane', '80-90', 'destroyed', '.', 'only', 'hospital', 'st.', 'croix', 'functioning', '.', 'needs', 'supply', 'desperately', '.'] \n",
      "\n",
      "TOKENIZED TXT with STOPWORDS:  ['un', 'report', 'leogane', '80-90', 'destroyed', '.', 'only', 'hospital', 'st.', 'croix', 'functioning', '.', 'needs', 'supply', 'desperately', '.'] \n",
      "\n",
      "\n",
      " -----------------------------------------------------------------------------\n",
      "UNCLEAN MSG:  says: west side of Haiti, rest of the country today and tonight\n",
      "\n",
      "\n",
      "TOKENIZED TXT:  ['say', ':', 'west', 'side', 'of', 'haiti', ',', 'rest', 'of', 'the', 'country', 'today', 'and', 'tonight'] \n",
      "\n",
      "TOKENIZED TXT with STOPWORDS:  ['say', ':', 'west', 'side', 'haiti', ',', 'rest', 'country', 'today', 'tonight'] \n",
      "\n",
      "\n",
      " -----------------------------------------------------------------------------\n",
      "UNCLEAN MSG:  Information about the National Palace-\n",
      "\n",
      "\n",
      "TOKENIZED TXT:  ['information', 'about', 'the', 'national', 'palace-'] \n",
      "\n",
      "TOKENIZED TXT with STOPWORDS:  ['information', 'national', 'palace-'] \n",
      "\n",
      "\n",
      " -----------------------------------------------------------------------------\n",
      "UNCLEAN MSG:  Storm at sacred heart of jesus\n",
      "\n",
      "\n",
      "TOKENIZED TXT:  ['storm', 'at', 'sacred', 'heart', 'of', 'jesus'] \n",
      "\n",
      "TOKENIZED TXT with STOPWORDS:  ['storm', 'sacred', 'heart', 'jesus'] \n",
      "\n",
      "\n",
      " -----------------------------------------------------------------------------\n",
      "UNCLEAN MSG:  Please, we need tents and water. We are in Silo, Thank you!\n",
      "\n",
      "\n",
      "TOKENIZED TXT:  ['please', ',', 'we', 'need', 'tent', 'and', 'water', '.', 'we', 'are', 'in', 'silo', ',', 'thank', 'you', '!'] \n",
      "\n",
      "TOKENIZED TXT with STOPWORDS:  ['please', ',', 'need', 'tent', 'water', '.', 'we', 'silo', ',', 'thank', '!'] \n",
      "\n",
      "\n",
      " -----------------------------------------------------------------------------\n",
      "UNCLEAN MSG:  I would like to receive the messages, thank you\n",
      "\n",
      "\n",
      "TOKENIZED TXT:  ['i', 'would', 'like', 'to', 'receive', 'the', 'message', ',', 'thank', 'you'] \n",
      "\n",
      "TOKENIZED TXT with STOPWORDS:  ['i', 'would', 'like', 'receive', 'message', ',', 'thank'] \n",
      "\n",
      "\n",
      " -----------------------------------------------------------------------------\n",
      "UNCLEAN MSG:  I am in Croix-des-Bouquets. We have health issues. They ( workers ) are in Santo 15. ( an area in Croix-des-Bouquets )\n",
      "\n",
      "\n",
      "TOKENIZED TXT:  ['i', 'am', 'in', 'croix-des-bouquets', '.', 'we', 'have', 'health', 'issue', '.', 'they', '(', 'worker', ')', 'are', 'in', 'santo', '15', '.', '(', 'an', 'area', 'in', 'croix-des-bouquets', ')'] \n",
      "\n",
      "TOKENIZED TXT with STOPWORDS:  ['i', 'croix-des-bouquets', '.', 'we', 'health', 'issue', '.', 'they', '(', 'worker', ')', 'santo', '15', '.', '(', 'area', 'croix-des-bouquets', ')'] \n",
      "\n",
      "\n",
      " -----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test tokenize function on first 10 messages from db, compare with and without stop words \n",
    "#looks like stop word removal takes too much meaning from messages\n",
    "#example, row 2  word 'over' was removed but that word creates value). \n",
    "#example, row 3, 'Looking for someone but no name' without stopwords changes meaning\n",
    "#For this ML exercise will NOT remove stop words\n",
    "X,y = ETL_Data()\n",
    "for message in X[0:10]:\n",
    "    tokens = tokenize_w_stopw(message)\n",
    "    print('UNCLEAN MSG: ', message)\n",
    "    print('\\n')\n",
    "    print('TOKENIZED TXT: ', tokens[0], '\\n')\n",
    "    print('TOKENIZED TXT with STOPWORDS: ', tokens[1], '\\n')\n",
    "    print('\\n','-----------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize function WITHOUT stop words (used for project below)\n",
    "def tokenize(text):\n",
    "    '''function to tokenize messages without using stop words\n",
    "    - tokenize words\n",
    "    - lemmatize words\n",
    "    - lowercase words\n",
    "    - remove whitespace\n",
    "    - append clean tokens\n",
    "    '''\n",
    "    #var for word tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    #instantiate lemmatizer (used to split into words and convert words to base forms)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # iterate through tokenized words and append tokenized and lemmatized words to clean_tokens var\n",
    "    clean_tokens = []\n",
    "    clean_tok_nostop = []\n",
    "    for tok in tokens:\n",
    "        \n",
    "        # lemmatize, normalize/put into lower case + remove leading/trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Pipeline. Chose multinomial Naive Bayes as 1st choice after reviewing the scikit algorithm cheat sheet:\n",
    "#https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "def nbpipe(X_train, X_test, y_train, y_test):\n",
    "    '''function to:\n",
    "    - pipeline using countvectorizer and tfidf transformers; classify with multinomial naive bayes\n",
    "    - fit pipeline with training data\n",
    "    - predict on test data\n",
    "    '''\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)), \n",
    "        ('tfidf', TfidfTransformer()), \n",
    "        ('clf', MultiOutputClassifier(MultinomialNB()))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # fit/train transformers and classifier\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # predict on test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    return pipeline, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to train and test\n",
    "X, y, ycol = ETL_Data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train pipeline\n",
    "pipeline, y_pred = nbpipe(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Evaluation (and some re-modeling to improve scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.77      1.00      0.87      6617\n",
      "               request       0.91      0.13      0.22      1450\n",
      "                 offer       0.00      0.00      0.00        41\n",
      "           aid_related       0.77      0.58      0.66      3607\n",
      "          medical_help       0.00      0.00      0.00       694\n",
      "      medical_products       1.00      0.00      0.00       441\n",
      "     search_and_rescue       0.00      0.00      0.00       242\n",
      "              security       0.00      0.00      0.00       170\n",
      "              military       0.00      0.00      0.00       265\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.00      0.00      0.00       542\n",
      "                  food       0.67      0.00      0.01       950\n",
      "               shelter       0.00      0.00      0.00       785\n",
      "              clothing       0.00      0.00      0.00       123\n",
      "                 money       0.00      0.00      0.00       195\n",
      "        missing_people       0.00      0.00      0.00       104\n",
      "              refugees       0.00      0.00      0.00       307\n",
      "                 death       0.00      0.00      0.00       411\n",
      "             other_aid       0.00      0.00      0.00      1160\n",
      "infrastructure_related       0.00      0.00      0.00       551\n",
      "             transport       0.00      0.00      0.00       385\n",
      "             buildings       0.00      0.00      0.00       449\n",
      "           electricity       0.00      0.00      0.00       180\n",
      "                 tools       0.00      0.00      0.00        55\n",
      "             hospitals       0.00      0.00      0.00        82\n",
      "                 shops       0.00      0.00      0.00        36\n",
      "           aid_centers       0.00      0.00      0.00       108\n",
      "  other_infrastructure       0.00      0.00      0.00       372\n",
      "       weather_related       0.88      0.31      0.45      2449\n",
      "                floods       0.00      0.00      0.00       737\n",
      "                 storm       1.00      0.00      0.01       809\n",
      "                  fire       0.00      0.00      0.00        95\n",
      "            earthquake       0.88      0.08      0.15       808\n",
      "                  cold       0.00      0.00      0.00       171\n",
      "         other_weather       0.00      0.00      0.00       442\n",
      "         direct_report       0.87      0.11      0.19      1681\n",
      "\n",
      "             micro avg       0.78      0.36      0.49     27514\n",
      "             macro avg       0.22      0.06      0.07     27514\n",
      "          weighted avg       0.56      0.36      0.36     27514\n",
      "           samples avg       0.73      0.40      0.47     27514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Multinomial NB score prior to gridsearch\n",
    "target_names = ycol\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))\n",
    "#Precision — What percent of your predictions were correct? Precision = TP/(TP + FP)\n",
    "#Recall — What percent of the positive cases did you catch? Recall = TP/(TP+FN)\n",
    "#F1 score — What percent of positive predictions were correct? F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "#Support is the number of actual occurrences of the class in the specified dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rerun pipeline without fitting (instantiated only)\n",
    "def nbpipe_improve(X_train, X_test, y_train, y_test):\n",
    "    '''function to:\n",
    "    - pipeline using countvectorizer and tfidf transformers; classify with multinomial naive bayes\n",
    "    '''\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)), \n",
    "        ('tfidf', TfidfTransformer()), \n",
    "        ('clf', MultiOutputClassifier(MultinomialNB()))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "#retrain pipeline\n",
    "pipeline = nbpipe_improve(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.172, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=None)]: Done   1 out of   1 | elapsed:    9.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.159, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=None)]: Done   2 out of   2 | elapsed:   18.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.166, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.175, total=   9.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.164, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.182, total=  11.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.167, total=  11.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.178, total=  11.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.184, total=  11.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.177, total=  11.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.209, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.200, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.197, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.202, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.206, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.234, total=  10.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.232, total=  10.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.222, total=  10.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.227, total=  10.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.230, total=  10.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.183, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.176, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.169, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.182, total=   9.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.181, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.219, total=  10.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.221, total=  10.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.213, total=  10.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.219, total=  10.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.225, total=  10.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.173, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.160, total=   9.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.168, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.176, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.165, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.182, total=  11.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.168, total=  11.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.178, total=  11.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.184, total=  11.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.178, total=  11.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.209, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.198, total=   9.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.197, total=   9.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.200, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.205, total=   9.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.230, total=  10.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.231, total=  10.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.222, total=  10.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.225, total=  10.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.229, total=  10.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.184, total=   9.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.174, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.169, total=   9.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.185, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.179, total=   9.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.220, total=  10.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.219, total=  10.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.213, total=  10.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.219, total=  10.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.224, total=  10.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.172, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.160, total=   9.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.168, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.176, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.164, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.182, total=  11.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.168, total=  11.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.178, total=  11.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.184, total=  11.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.178, total=  11.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.206, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.198, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.195, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.200, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.203, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.233, total=  10.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.231, total=  10.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.222, total=  10.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.225, total=  10.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.230, total=  10.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.182, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.175, total=   9.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.169, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.183, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.180, total=   9.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.215, total=  10.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.217, total=  10.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.212, total=  10.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.219, total=  10.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.223, total=  10.7s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.178, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.166, total=   9.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.171, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.179, total=   9.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.171, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.189, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.172, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.181, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.189, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.183, total=  11.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.193, total=   9.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.187, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.183, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.190, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.191, total=   9.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.221, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.221, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.214, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.215, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.223, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.179, total=   9.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.167, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.164, total=   9.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.178, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.167, total=   9.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.209, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.205, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.201, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.210, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.213, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.183, total=   9.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.171, total=   9.3s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.178, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.182, total=   9.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.177, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.192, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.177, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.185, total=  11.3s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.192, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.187, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.196, total=   9.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.182, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.179, total=   9.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.188, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.187, total=   9.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.217, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.216, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.210, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.219, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.223, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.182, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.165, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.168, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.179, total=   9.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.170, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.210, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.198, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.197, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.210, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.210, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.184, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.172, total=   9.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.179, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.183, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.178, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.192, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.178, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.187, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.193, total=  11.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.188, total=  11.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.194, total=   9.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.182, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.176, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.188, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.185, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.213, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.217, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.208, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.217, total=  10.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.221, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.182, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.165, total=   9.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.170, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.180, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.172, total=   9.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.207, total=  10.4s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.202, total=  10.4s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.194, total=  10.4s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.209, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.206, total=  10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=None)]: Done 180 out of 180 | elapsed: 30.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': 5000, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "#parameters for gridsearch + model fitting; then print best parameters from analysis\n",
    "parameters = {\n",
    "\n",
    "#for naive bayes    \n",
    "        'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'vect__max_features': (None, 5000, 10000),\n",
    "        'tfidf__use_idf': (True, False)\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, verbose=3)\n",
    "\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning model per best_params\n",
    "def nbpipe_tuned(X_train, X_test, y_train, y_test):\n",
    "    '''function to:\n",
    "    - pipeline using countvectorizer and tfidf transformers; \n",
    "    - pipeline classify with multinomial naive bayes, best params after gridsearch analysis\n",
    "    - fit pipeline with training data\n",
    "    - predict on test data\n",
    "    '''\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize, max_df = 0.5, max_features = 5000, ngram_range=(1, 2))), \n",
    "        ('tfidf', TfidfTransformer(use_idf = True)), \n",
    "        ('clf', MultiOutputClassifier(MultinomialNB()))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # fit/train transformers and classifier on tuned model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # predict on test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    return pipeline, y_pred\n",
    "\n",
    "#train pipeline\n",
    "pipeline, y_pred = nbpipe_tuned(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.84      0.93      0.88      6617\n",
      "               request       0.71      0.66      0.68      1450\n",
      "                 offer       0.00      0.00      0.00        41\n",
      "           aid_related       0.75      0.65      0.69      3607\n",
      "          medical_help       0.61      0.13      0.21       694\n",
      "      medical_products       0.65      0.17      0.27       441\n",
      "     search_and_rescue       0.00      0.00      0.00       242\n",
      "              security       0.00      0.00      0.00       170\n",
      "              military       0.63      0.05      0.08       265\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.78      0.27      0.40       542\n",
      "                  food       0.77      0.45      0.57       950\n",
      "               shelter       0.82      0.25      0.38       785\n",
      "              clothing       0.58      0.12      0.20       123\n",
      "                 money       1.00      0.01      0.01       195\n",
      "        missing_people       0.00      0.00      0.00       104\n",
      "              refugees       1.00      0.00      0.01       307\n",
      "                 death       0.86      0.19      0.32       411\n",
      "             other_aid       0.66      0.03      0.05      1160\n",
      "infrastructure_related       0.45      0.01      0.02       551\n",
      "             transport       0.93      0.03      0.07       385\n",
      "             buildings       0.90      0.06      0.11       449\n",
      "           electricity       0.00      0.00      0.00       180\n",
      "                 tools       0.00      0.00      0.00        55\n",
      "             hospitals       0.00      0.00      0.00        82\n",
      "                 shops       0.00      0.00      0.00        36\n",
      "           aid_centers       0.00      0.00      0.00       108\n",
      "  other_infrastructure       0.00      0.00      0.00       372\n",
      "       weather_related       0.75      0.62      0.68      2449\n",
      "                floods       0.75      0.31      0.44       737\n",
      "                 storm       0.71      0.38      0.50       809\n",
      "                  fire       0.00      0.00      0.00        95\n",
      "            earthquake       0.86      0.41      0.56       808\n",
      "                  cold       0.00      0.00      0.00       171\n",
      "         other_weather       0.41      0.02      0.03       442\n",
      "         direct_report       0.65      0.56      0.60      1681\n",
      "\n",
      "             micro avg       0.78      0.51      0.61     27514\n",
      "             macro avg       0.47      0.17      0.22     27514\n",
      "          weighted avg       0.71      0.51      0.54     27514\n",
      "           samples avg       0.64      0.46      0.49     27514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Multinomial NB score with tuning.\n",
    "#F1 Score and accuracy score better with tuned model(below)\n",
    "target_names = ycol\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))\n",
    "#Precision — What percent of your predictions were correct? Precision = TP/(TP + FP)\n",
    "#Recall — What percent of the positive cases did you catch? Recall = TP/(TP+FN)\n",
    "#F1 score — What percent of positive predictions were correct? F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "#Support is the number of actual occurrences of the class in the specified dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIPELINE, KNN ONLY\n",
    "def knn_pipe(X_train, X_test, y_train, y_test):\n",
    "    '''function to:\n",
    "    - pipeline using countvectorizer and tfidf transformers; classify with K nearest neighbor\n",
    "    - fit pipeline with training data\n",
    "    - predict on test data\n",
    "    '''\n",
    "    pipeline2 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize, max_df = 0.5, max_features = 5000, ngram_range=(1, 2))), \n",
    "        ('tfidf', TfidfTransformer(use_idf = True)), \n",
    "        ('clf', MultiOutputClassifier(KNeighborsClassifier(n_neighbors = 5)))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # fit/train transformers and classifier\n",
    "    pipeline2.fit(X_train, y_train)\n",
    "    # predict on test data\n",
    "    y_pred2 = pipeline2.predict(X_test)\n",
    "    \n",
    "    return pipeline2, y_pred2\n",
    "\n",
    "#train pipeline\n",
    "pipeline2, y_pred2 = knn_pipe(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.78      0.99      0.87      6617\n",
      "               request       0.84      0.06      0.12      1450\n",
      "                 offer       0.00      0.00      0.00        41\n",
      "           aid_related       0.72      0.03      0.06      3607\n",
      "          medical_help       0.83      0.01      0.01       694\n",
      "      medical_products       0.57      0.02      0.04       441\n",
      "     search_and_rescue       0.00      0.00      0.00       242\n",
      "              security       0.00      0.00      0.00       170\n",
      "              military       0.00      0.00      0.00       265\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.88      0.04      0.07       542\n",
      "                  food       0.84      0.06      0.11       950\n",
      "               shelter       0.78      0.02      0.04       785\n",
      "              clothing       0.40      0.02      0.03       123\n",
      "                 money       0.25      0.01      0.01       195\n",
      "        missing_people       0.00      0.00      0.00       104\n",
      "              refugees       0.67      0.01      0.01       307\n",
      "                 death       0.80      0.03      0.06       411\n",
      "             other_aid       0.35      0.01      0.01      1160\n",
      "infrastructure_related       0.67      0.01      0.01       551\n",
      "             transport       0.50      0.00      0.01       385\n",
      "             buildings       0.88      0.02      0.03       449\n",
      "           electricity       0.00      0.00      0.00       180\n",
      "                 tools       0.00      0.00      0.00        55\n",
      "             hospitals       0.00      0.00      0.00        82\n",
      "                 shops       0.00      0.00      0.00        36\n",
      "           aid_centers       0.00      0.00      0.00       108\n",
      "  other_infrastructure       0.40      0.01      0.01       372\n",
      "       weather_related       0.76      0.07      0.13      2449\n",
      "                floods       0.91      0.01      0.03       737\n",
      "                 storm       0.70      0.04      0.08       809\n",
      "                  fire       0.00      0.00      0.00        95\n",
      "            earthquake       0.76      0.13      0.22       808\n",
      "                  cold       0.50      0.01      0.01       171\n",
      "         other_weather       0.75      0.01      0.01       442\n",
      "         direct_report       0.81      0.05      0.10      1681\n",
      "\n",
      "             micro avg       0.78      0.27      0.40     27514\n",
      "             macro avg       0.45      0.05      0.06     27514\n",
      "          weighted avg       0.70      0.27      0.26     27514\n",
      "           samples avg       0.75      0.34      0.41     27514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ycol\n",
    "print(classification_report(y_test, y_pred2, target_names=target_names, zero_division=0))\n",
    "#Precision — What percent of your predictions were correct? Precision = TP/(TP + FP)\n",
    "#Recall — What percent of the positive cases did you catch? Recall = TP/(TP+FN)\n",
    "#F1 score — What percent of positive predictions were correct? F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "#Support is the number of actual occurrences of the class in the specified dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinominal Naive Bayes with tuned parameters peforms better than the K Neareset Neighbor model. Micro ## Average F1 (accuracy) and recall scords much better with tuned MNB. Precision similiar across all models.\n",
    "## Will stick with fine tuned MNB as model of choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
